{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import utils.state as game_state\n",
    "import utils.static as game_static\n",
    "from utils.ModifiedTensorBoard import ModifiedTensorBoard\n",
    "from utils.PositionalController import PositionalController\n",
    "from game import AbstractGame\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.state as game_state\n",
    "import utils.static as game_static\n",
    "from utils.ModifiedTensorBoard import ModifiedTensorBoard\n",
    "import pygame\n",
    "\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "REPLAY_MEMORY_SIZE = 8_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 500  \n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 10  # Terminal states (end of episodes)\n",
    "\n",
    "\n",
    "\n",
    "class PositionalController:\n",
    "    def __init__(self, model_name,lr,discount,number_of_states,number_of_actions) -> None:\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.number_of_states = number_of_states\n",
    "        self.number_of_actions = number_of_actions\n",
    "        \n",
    "        self.model = self.create_model() # main model\n",
    "\n",
    "        self.target_model = self.create_model() # target model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.target_update_counter = 0 #when to update target network with main network's weights\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE) # array with last n steps for minibatch training\n",
    "\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(model_name, int(time.time()))) #logging\n",
    "\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(128, input_shape=(self.number_of_states,), activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(self.number_of_actions, activation='linear'),\n",
    "        ])        \n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.lr))\n",
    "        return model\n",
    "        \n",
    "    def get_features(self, state):\n",
    "        player_info = [\n",
    "            state.player_entity.entity.height/game_static.MAX_ENTITY_SIZE,\n",
    "            state.player_entity.entity.width/game_static.MAX_ENTITY_SIZE,\n",
    "            state.player_entity.friction,\n",
    "            state.player_entity.acc_factor,\n",
    "             \n",
    "            state.player_entity.entity.x/game_static.GAME_WIDTH,  \n",
    "            state.player_entity.entity.y/game_static.GAME_HEIGHT,\n",
    "            state.player_entity.velocity.x,\n",
    "            state.player_entity.velocity.y\n",
    "        ]\n",
    "        goal_info = [\n",
    "            state.goal_entity.x/game_static.GAME_WIDTH,\n",
    "            state.goal_entity.y/game_static.GAME_HEIGHT,\n",
    "            state.goal_entity.height/game_static.MAX_ENTITY_SIZE,\n",
    "            state.goal_entity.width/game_static.MAX_ENTITY_SIZE,\n",
    "            # distance to goal\n",
    "            (\n",
    "                np.sqrt(\n",
    "                    (state.player_entity.entity.x - state.goal_entity.x)**2 \n",
    "                    + (state.player_entity.entity.y - state.goal_entity.y)**2\n",
    "                ) / game_static.GAME_DIAGONAL\n",
    "            )\n",
    "        ]\n",
    "        enemy_info = [\n",
    "            [\n",
    "                enemy_entity.entity.x/game_static.GAME_WIDTH,\n",
    "                enemy_entity.entity.y/game_static.GAME_HEIGHT,\n",
    "                enemy_entity.entity.height/game_static.MAX_ENTITY_SIZE,\n",
    "                enemy_entity.entity.width/game_static.MAX_ENTITY_SIZE,\n",
    "                enemy_entity.velocity.x,\n",
    "                enemy_entity.velocity.y,\n",
    "                # distance from the player to the enemy\n",
    "                np.sqrt(\n",
    "                        (state.player_entity.entity.x - enemy_entity.entity.x)**2 \n",
    "                        + (state.player_entity.entity.y - enemy_entity.entity.y)**2\n",
    "                ) / game_static.GAME_DIAGONAL\n",
    "            ]\n",
    "            for enemy_entity in state.enemy_collection\n",
    "        ]\n",
    "        return player_info, goal_info, enemy_info\n",
    "    \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        player_info, goal_info, enemy_info = self.get_features(state)\n",
    "        state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "        \n",
    "        # deciding which action to take\n",
    "        action = self.model.predict(np.array(state_tensor).reshape(-1, self.number_of_states),verbose=0)[0]\n",
    "        return game_state.GameActions(np.argmax(action))\n",
    "\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        # transition = (observation space, action, reward, new observation space, done)\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self,done):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE: # starting after certain number of steps\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        current_states = tf.convert_to_tensor([transition[0] for transition in minibatch]) # get current states from minibatch\n",
    "        current_qs_list = self.model.predict(current_states,verbose=0) # query the model for Q values\n",
    "\n",
    "        new_states = np.array([transition[3] for transition in minibatch]) # get future states from minibatch\n",
    "        future_qs_list = self.target_model.predict(new_states, verbose=0) # use target model to predict future states\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "\n",
    "            \n",
    "            if not done: # if the game hasn't ended, get new q from future states\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + self.discount * max_future_q\n",
    "            else:\n",
    "                new_q = reward # otherwise set it to 0\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # fit the main model using the batch\n",
    "        self.model.fit(\n",
    "            np.array(X), np.array(y),\n",
    "            batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False,\n",
    "            callbacks=[self.tensorboard])\n",
    "\n",
    "        if done: # if done -> episode is over\n",
    "            self.target_update_counter += 1 # don't update weights of target model every episode\n",
    "\n",
    "        # after a set number of episodes update the target model\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'PC1'\n",
    "LR = 1e-3\n",
    "DISCOUNT = 0.99\n",
    "MIN_REWARD = -50  # For model save\n",
    "EPISODES = 200\n",
    "\n",
    "NUMBER_OF_STATES = (\n",
    "    game_static.ENEMY_COUNT * 6 # each enemy has 6 features: x,y position and velocity, as well as height and width\n",
    "    + game_static.ENEMY_COUNT # distance from the player to the enemy\n",
    "    + 8 # player has 8 features: x,y position and velocity, height and width, friction, acceleration factor\n",
    "    + 4 # goal has 4 features: x,y position, height and width\n",
    "    + 1 # distance to goal\n",
    "    \n",
    ")\n",
    "NUMBER_OF_ACTIONS = 9\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # decayed over time\n",
    "EPSILON_DECAY = 0.98\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 10  # episodes\n",
    "\n",
    "agent = PositionalController(MODEL_NAME,LR, DISCOUNT,NUMBER_OF_STATES,NUMBER_OF_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|######9   | 139/200 [27:39<18:55, 18.62s/episodes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC1___-38.80max_-127.75avg_-180.16min__1732539782.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC1___-38.80max_-127.75avg_-180.16min__1732539782.model\\assets\n",
      "100%|##########| 200/200 [43:12<00:00, 12.96s/episodes]\n"
     ]
    }
   ],
   "source": [
    "my_game = AbstractGame(agent)\n",
    "\n",
    "ep_rewards = []\n",
    "ep_goals = []\n",
    "ep_steps = []\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    goals_scored = 0 \n",
    "    steps = 0\n",
    "    step_when_last_goal_scored = 0\n",
    "    # get initial state\n",
    "    my_game.reset()\n",
    "    current_state = my_game.get_current_state()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    while not my_game.done:\n",
    "        episode_step += 1\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            action_enum = agent.get_action(current_state)\n",
    "        else:\n",
    "            action_enum = game_state.GameActions(\n",
    "                np.random.randint(0, NUMBER_OF_ACTIONS)\n",
    "            )\n",
    " \n",
    "        # Do action and get what is the current_observation\n",
    "        my_game.update_frame(action_enum)\n",
    "        \n",
    "        new_state = my_game.get_current_state()        \n",
    "        player_info, goal_info, enemy_info = agent.get_features(new_state)\n",
    "                \n",
    "        is_goal_scored = new_state.current_observation.value == 1\n",
    "        if is_goal_scored:\n",
    "            goals_scored += 1\n",
    "            step_when_last_goal_scored = episode_step \n",
    "            goal_scored_reward = 50 \n",
    "        else:\n",
    "            goal_scored_reward = 0\n",
    "        enemy_attacked_penalty = -100 if new_state.current_observation.value == -1 else 0\n",
    "        goal_distance_penalty = -goal_info[-1]\n",
    "        time_penalty = -0.01  \n",
    "\n",
    "        reward = goal_scored_reward + goal_distance_penalty + time_penalty + enemy_attacked_penalty\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        # update replay memory and train model(s)\n",
    "        player_info, goal_info, enemy_info = agent.get_features(current_state)\n",
    "        current_state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "\n",
    "        player_info, goal_info, enemy_info = agent.get_features(new_state)\n",
    "        new_state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "\n",
    "        agent.update_replay_memory((current_state_tensor, action_enum.value, reward, new_state_tensor, my_game.done))\n",
    "        agent.train(my_game.done)\n",
    "        \n",
    "        \n",
    "        current_state = new_state\n",
    "        my_game.__innerState = current_state\n",
    "        if episode_step - step_when_last_goal_scored > 250 and goals_scored == 0:\n",
    "            episode_reward -= 100 # for tensorboard logging, so that there are no falsely high rewards with no goals scored\n",
    "            break # no progress, and hasn't scored a goal in a while\n",
    "        # my_game.draw_frame()\n",
    "    \n",
    "        \n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    ep_goals.append(goals_scored)\n",
    "    ep_steps.append(episode_step)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        average_goals = sum(ep_goals[-AGGREGATE_STATS_EVERY:])/len(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        min_goals = min(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        max_goals = max(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        average_steps = sum(ep_steps[-AGGREGATE_STATS_EVERY:])/len(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        min_steps = min(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        max_steps = max(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        agent.tensorboard.update_stats(\n",
    "            epsilon=epsilon, reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward,\n",
    "            goals_avg=average_goals, goals_min=min_goals, goals_max=max_goals,\n",
    "            steps_avg=average_steps, steps_min=min_steps, steps_max=max_steps\n",
    "        )\n",
    "\n",
    "        if max_reward >= MIN_REWARD: # save model when it's good enough\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "\n",
    "    # decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "        \n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'PC2'\n",
    "LR = 1e-3\n",
    "DISCOUNT = 0.99\n",
    "MIN_REWARD = -75  # For model save\n",
    "EPISODES = 300\n",
    "\n",
    "NUMBER_OF_STATES = (\n",
    "    game_static.ENEMY_COUNT * 6 # each enemy has 6 features: x,y position and velocity, as well as height and width\n",
    "    + game_static.ENEMY_COUNT # distance from the player to the enemy\n",
    "    + 8 # player has 8 features: x,y position and velocity, height and width, friction, acceleration factor\n",
    "    + 4 # goal has 4 features: x,y position, height and width\n",
    "    + 1 # distance to goal\n",
    "    \n",
    ")\n",
    "NUMBER_OF_ACTIONS = 9\n",
    "\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # decayed over time\n",
    "EPSILON_DECAY = 0.99\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 10  # episodes\n",
    "\n",
    "agent = PositionalController(MODEL_NAME,LR, DISCOUNT,NUMBER_OF_STATES,NUMBER_OF_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|##9       | 89/300 [11:01<17:44,  5.05s/episodes]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-60.61max_-118.46avg_-220.43min__1732545769.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-60.61max_-118.46avg_-220.43min__1732545769.model\\assets\n",
      " 36%|###6      | 109/300 [14:24<28:30,  8.95s/episodes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-62.03max_-114.38avg_-176.37min__1732545968.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-62.03max_-114.38avg_-176.37min__1732545968.model\\assets\n",
      " 51%|#####1    | 154/300 [22:47<30:49, 12.67s/episodes]  "
     ]
    }
   ],
   "source": [
    "my_game = AbstractGame(agent)\n",
    "\n",
    "ep_rewards = []\n",
    "ep_goals = []\n",
    "ep_steps = []\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    goals_scored = 0 \n",
    "    steps = 0\n",
    "    step_when_last_goal_scored = 0\n",
    "    # get initial state\n",
    "    my_game.reset()\n",
    "    current_state = my_game.get_current_state()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    while not my_game.done:\n",
    "        episode_step += 1\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            action_enum = agent.get_action(current_state)\n",
    "        else:\n",
    "            action_enum = game_state.GameActions(\n",
    "                np.random.randint(0, NUMBER_OF_ACTIONS)\n",
    "            )\n",
    " \n",
    "        # Do action and get what is the current_observation\n",
    "        my_game.update_frame(action_enum)\n",
    "        \n",
    "        new_state = my_game.get_current_state()        \n",
    "        player_info, goal_info, enemy_info = agent.get_features(new_state)\n",
    "                \n",
    "        is_goal_scored = new_state.current_observation.value == 1\n",
    "        if is_goal_scored:\n",
    "            goals_scored += 1\n",
    "            step_when_last_goal_scored = episode_step \n",
    "            goal_scored_reward = 50 \n",
    "        else:\n",
    "            goal_scored_reward = 0\n",
    "        enemy_attacked_penalty = -100 if new_state.current_observation.value == -1 else 0\n",
    "        goal_distance_penalty = -goal_info[-1]\n",
    "        time_penalty = -0.01  \n",
    "\n",
    "        reward = goal_scored_reward + goal_distance_penalty + time_penalty + enemy_attacked_penalty\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        # update replay memory and train model(s)\n",
    "        player_info, goal_info, enemy_info = agent.get_features(current_state)\n",
    "        current_state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "\n",
    "        player_info, goal_info, enemy_info = agent.get_features(new_state)\n",
    "        new_state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "\n",
    "        agent.update_replay_memory((current_state_tensor, action_enum.value, reward, new_state_tensor, my_game.done))\n",
    "        agent.train(my_game.done)\n",
    "        \n",
    "        \n",
    "        current_state = new_state\n",
    "        my_game.__innerState = current_state\n",
    "        if episode_step - step_when_last_goal_scored > 250 and goals_scored == 0:\n",
    "            episode_reward -= 100 # for tensorboard logging, so that there are no falsely high rewards with no goals scored\n",
    "            break # no progress, and hasn't scored a goal in a while\n",
    "        # my_game.draw_frame()\n",
    "    \n",
    "        \n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    ep_goals.append(goals_scored)\n",
    "    ep_steps.append(episode_step)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        average_goals = sum(ep_goals[-AGGREGATE_STATS_EVERY:])/len(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        min_goals = min(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        max_goals = max(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        average_steps = sum(ep_steps[-AGGREGATE_STATS_EVERY:])/len(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        min_steps = min(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        max_steps = max(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        agent.tensorboard.update_stats(\n",
    "            epsilon=epsilon, reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward,\n",
    "            goals_avg=average_goals, goals_min=min_goals, goals_max=max_goals,\n",
    "            steps_avg=average_steps, steps_min=min_steps, steps_max=max_steps\n",
    "        )\n",
    "\n",
    "        if max_reward >= MIN_REWARD: # save model when it's good enough\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "\n",
    "    # decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "        \n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
