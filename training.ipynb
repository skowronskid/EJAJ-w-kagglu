{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import utils.state as game_state\n",
    "import utils.static as game_static\n",
    "from utils.ModifiedTensorBoard import ModifiedTensorBoard\n",
    "from utils.PositionalController import PositionalController\n",
    "from game import AbstractGame\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.state as game_state\n",
    "import utils.static as game_static\n",
    "from utils.ModifiedTensorBoard import ModifiedTensorBoard\n",
    "import pygame\n",
    "\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "REPLAY_MEMORY_SIZE = 8_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 500  \n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 10  # Terminal states (end of episodes)\n",
    "\n",
    "\n",
    "\n",
    "class PositionalController:\n",
    "    def __init__(self, model_name,lr,discount,number_of_states,number_of_actions) -> None:\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.number_of_states = number_of_states\n",
    "        self.number_of_actions = number_of_actions\n",
    "        \n",
    "        self.model = self.create_model() # main model\n",
    "\n",
    "        self.target_model = self.create_model() # target model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.target_update_counter = 0 #when to update target network with main network's weights\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE) # array with last n steps for minibatch training\n",
    "\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(model_name, int(time.time()))) #logging\n",
    "\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(128, input_shape=(self.number_of_states,), activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(self.number_of_actions, activation='linear'),\n",
    "        ])        \n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.lr))\n",
    "        return model\n",
    "        \n",
    "    def get_features(self, state):\n",
    "        player_info = [\n",
    "            state.player_entity.entity.height/game_static.MAX_ENTITY_SIZE,\n",
    "            state.player_entity.entity.width/game_static.MAX_ENTITY_SIZE,\n",
    "            state.player_entity.friction,\n",
    "            state.player_entity.acc_factor,\n",
    "             \n",
    "            state.player_entity.entity.x/game_static.GAME_WIDTH,  \n",
    "            state.player_entity.entity.y/game_static.GAME_HEIGHT,\n",
    "            state.player_entity.velocity.x,\n",
    "            state.player_entity.velocity.y\n",
    "        ]\n",
    "        goal_info = [\n",
    "            state.goal_entity.x/game_static.GAME_WIDTH,\n",
    "            state.goal_entity.y/game_static.GAME_HEIGHT,\n",
    "            state.goal_entity.height/game_static.MAX_ENTITY_SIZE,\n",
    "            state.goal_entity.width/game_static.MAX_ENTITY_SIZE,\n",
    "            # distance to goal\n",
    "            (\n",
    "                np.sqrt(\n",
    "                    (state.player_entity.entity.x - state.goal_entity.x)**2 \n",
    "                    + (state.player_entity.entity.y - state.goal_entity.y)**2\n",
    "                ) / game_static.GAME_DIAGONAL\n",
    "            )\n",
    "        ]\n",
    "        enemy_info = [\n",
    "            [\n",
    "                enemy_entity.entity.x/game_static.GAME_WIDTH,\n",
    "                enemy_entity.entity.y/game_static.GAME_HEIGHT,\n",
    "                enemy_entity.entity.height/game_static.MAX_ENTITY_SIZE,\n",
    "                enemy_entity.entity.width/game_static.MAX_ENTITY_SIZE,\n",
    "                enemy_entity.velocity.x,\n",
    "                enemy_entity.velocity.y,\n",
    "                # distance from the player to the enemy\n",
    "                np.sqrt(\n",
    "                        (state.player_entity.entity.x - enemy_entity.entity.x)**2 \n",
    "                        + (state.player_entity.entity.y - enemy_entity.entity.y)**2\n",
    "                ) / game_static.GAME_DIAGONAL\n",
    "            ]\n",
    "            for enemy_entity in state.enemy_collection\n",
    "        ]\n",
    "        return player_info, goal_info, enemy_info\n",
    "    \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        player_info, goal_info, enemy_info = self.get_features(state)\n",
    "        state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "        \n",
    "        # deciding which action to take\n",
    "        action = self.model.predict(np.array(state_tensor).reshape(-1, self.number_of_states),verbose=0)[0]\n",
    "        return game_state.GameActions(np.argmax(action))\n",
    "\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        # transition = (observation space, action, reward, new observation space, done)\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self,done):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE: # starting after certain number of steps\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        current_states = tf.convert_to_tensor([transition[0] for transition in minibatch]) # get current states from minibatch\n",
    "        current_qs_list = self.model.predict(current_states,verbose=0) # query the model for Q values\n",
    "\n",
    "        new_states = np.array([transition[3] for transition in minibatch]) # get future states from minibatch\n",
    "        future_qs_list = self.target_model.predict(new_states, verbose=0) # use target model to predict future states\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "\n",
    "            \n",
    "            if not done: # if the game hasn't ended, get new q from future states\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + self.discount * max_future_q\n",
    "            else:\n",
    "                new_q = reward # otherwise set it to 0\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # fit the main model using the batch\n",
    "        self.model.fit(\n",
    "            np.array(X), np.array(y),\n",
    "            batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False,\n",
    "            callbacks=[self.tensorboard])\n",
    "\n",
    "        if done: # if done -> episode is over\n",
    "            self.target_update_counter += 1 # don't update weights of target model every episode\n",
    "\n",
    "        # after a set number of episodes update the target model\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'PC3'\n",
    "LR = 1e-3\n",
    "DISCOUNT = 0.99\n",
    "MIN_REWARD = -50  # For model save\n",
    "EPISODES = 200\n",
    "\n",
    "NUMBER_OF_STATES = (\n",
    "    game_static.ENEMY_COUNT * 6 # each enemy has 6 features: x,y position and velocity, as well as height and width\n",
    "    + game_static.ENEMY_COUNT # distance from the player to the enemy\n",
    "    + 8 # player has 8 features: x,y position and velocity, height and width, friction, acceleration factor\n",
    "    # + 6\n",
    "    + 4 # goal has 4 features: x,y position, height and width\n",
    "    + 1 # distance to goal\n",
    "    \n",
    ")\n",
    "NUMBER_OF_ACTIONS = 9\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # decayed over time\n",
    "EPSILON_DECAY = 0.995\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 10  # episodes\n",
    "\n",
    "agent = PositionalController(MODEL_NAME,LR, DISCOUNT,NUMBER_OF_STATES,NUMBER_OF_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|2         | 5/200 [02:33<1:39:30, 30.62s/episodes]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m new_state_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([\u001b[38;5;28mfloat\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m feature_list \u001b[38;5;129;01min\u001b[39;00m [player_info, goal_info, \u001b[38;5;241m*\u001b[39menemy_info] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feature_list])\n\u001b[0;32m     64\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate_replay_memory((current_state_tensor, action_enum\u001b[38;5;241m.\u001b[39mvalue, reward, new_state_tensor, my_game\u001b[38;5;241m.\u001b[39mdone))\n\u001b[1;32m---> 65\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_game\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m current_state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[0;32m     69\u001b[0m my_game\u001b[38;5;241m.\u001b[39m__innerState \u001b[38;5;241m=\u001b[39m current_state\n",
      "Cell \u001b[1;32mIn[2], line 126\u001b[0m, in \u001b[0;36mPositionalController.train\u001b[1;34m(self, done)\u001b[0m\n\u001b[0;32m    123\u001b[0m current_qs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(current_states,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# query the model for Q values\u001b[39;00m\n\u001b[0;32m    125\u001b[0m new_states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([transition[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m minibatch]) \u001b[38;5;66;03m# get future states from minibatch\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m future_qs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# use target model to predict future states\u001b[39;00m\n\u001b[0;32m    128\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    129\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\skowr\\miniconda3\\envs\\py310_tf\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\skowr\\miniconda3\\envs\\py310_tf\\lib\\site-packages\\keras\\src\\engine\\training.py:2627\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_tuner\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m   2626\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2629\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[1;32mc:\\Users\\skowr\\miniconda3\\envs\\py310_tf\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1341\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1341\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1342\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[0;32m   1343\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\skowr\\miniconda3\\envs\\py310_tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:496\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    495\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\skowr\\miniconda3\\envs\\py310_tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    701\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    704\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 705\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\skowr\\miniconda3\\envs\\py310_tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    741\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    742\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    743\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 744\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\skowr\\miniconda3\\envs\\py310_tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3451\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3450\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3451\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3452\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3454\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_game = AbstractGame(agent,training=True)\n",
    "\n",
    "ep_rewards = []\n",
    "ep_goals = []\n",
    "ep_steps = []\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    goals_scored = 0 \n",
    "    steps = 0\n",
    "    step_when_last_goal_scored = 0\n",
    "    previous_goal_distance = 1\n",
    "    # get initial state\n",
    "    my_game.reset()\n",
    "    current_state = my_game.get_current_state()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    while not my_game.done:\n",
    "        episode_step += 1\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            action_enum = agent.get_action(current_state)\n",
    "        else:\n",
    "            action_enum = game_state.GameActions(\n",
    "                np.random.randint(0, NUMBER_OF_ACTIONS)\n",
    "            )\n",
    " \n",
    "        # Do action and get what is the current_observation\n",
    "        my_game.update_frame(action_enum)\n",
    "        \n",
    "        new_state = my_game.get_current_state()        \n",
    "        player_info, goal_info, enemy_info = agent.get_features(new_state)\n",
    "                \n",
    "        is_goal_scored = new_state.current_observation.value == 1\n",
    "        if is_goal_scored:\n",
    "            goals_scored += 1\n",
    "            step_when_last_goal_scored = episode_step \n",
    "            goal_scored_reward = 200 \n",
    "        else:\n",
    "            goal_scored_reward = 0\n",
    "        enemy_attacked_penalty = -100 if new_state.current_observation.value == -1 else 0\n",
    "        goal_distance_penalty = -goal_info[2] * 2 if -goal_info[2] < previous_goal_distance else -2\n",
    "        previous_goal_distance = -goal_info[2]\n",
    "        time_penalty = -0.01  \n",
    "\n",
    "        reward = goal_scored_reward + goal_distance_penalty + time_penalty + enemy_attacked_penalty\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        # update replay memory and train model(s)\n",
    "        player_info, goal_info, enemy_info = agent.get_features(current_state)\n",
    "        current_state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "\n",
    "        player_info, goal_info, enemy_info = agent.get_features(new_state)\n",
    "        new_state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "\n",
    "        agent.update_replay_memory((current_state_tensor, action_enum.value, reward, new_state_tensor, my_game.done))\n",
    "        agent.train(my_game.done)\n",
    "        \n",
    "        \n",
    "        current_state = new_state\n",
    "        my_game.__innerState = current_state\n",
    "        if episode_step - step_when_last_goal_scored > 250 and goals_scored == 0:\n",
    "            episode_reward -= 100 # for tensorboard logging, so that there are no falsely high rewards with no goals scored\n",
    "            break # no progress, and hasn't scored a goal in a while\n",
    "        # my_game.draw_frame()\n",
    "    \n",
    "        \n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    ep_goals.append(goals_scored)\n",
    "    ep_steps.append(episode_step)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        average_goals = sum(ep_goals[-AGGREGATE_STATS_EVERY:])/len(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        min_goals = min(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        max_goals = max(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        average_steps = sum(ep_steps[-AGGREGATE_STATS_EVERY:])/len(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        min_steps = min(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        max_steps = max(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        agent.tensorboard.update_stats(\n",
    "            epsilon=epsilon, reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward,\n",
    "            goals_avg=average_goals, goals_min=min_goals, goals_max=max_goals,\n",
    "            steps_avg=average_steps, steps_min=min_steps, steps_max=max_steps\n",
    "        )\n",
    "\n",
    "        if max_reward >= MIN_REWARD: # save model when it's good enough\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "\n",
    "    # decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'PC2'\n",
    "LR = 1e-3\n",
    "DISCOUNT = 0.99\n",
    "MIN_REWARD = -75  # For model save\n",
    "EPISODES = 300\n",
    "\n",
    "NUMBER_OF_STATES = (\n",
    "    game_static.ENEMY_COUNT * 6 # each enemy has 6 features: x,y position and velocity, as well as height and width\n",
    "    + game_static.ENEMY_COUNT # distance from the player to the enemy\n",
    "    + 8 # player has 8 features: x,y position and velocity, height and width, friction, acceleration factor\n",
    "    + 4 # goal has 4 features: x,y position, height and width\n",
    "    + 1 # distance to goal\n",
    "    \n",
    ")\n",
    "NUMBER_OF_ACTIONS = 9\n",
    "\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # decayed over time\n",
    "EPSILON_DECAY = 0.99\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 10  # episodes\n",
    "\n",
    "agent = PositionalController(MODEL_NAME,LR, DISCOUNT,NUMBER_OF_STATES,NUMBER_OF_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|##9       | 89/300 [11:01<17:44,  5.05s/episodes]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-60.61max_-118.46avg_-220.43min__1732545769.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-60.61max_-118.46avg_-220.43min__1732545769.model\\assets\n",
      " 36%|###6      | 109/300 [14:24<28:30,  8.95s/episodes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-62.03max_-114.38avg_-176.37min__1732545968.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-62.03max_-114.38avg_-176.37min__1732545968.model\\assets\n",
      " 56%|#####6    | 169/300 [27:10<22:13, 10.18s/episodes]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-74.48max_-135.56avg_-225.39min__1732546771.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-74.48max_-135.56avg_-225.39min__1732546771.model\\assets\n",
      " 73%|#######3  | 219/300 [41:49<29:28, 21.84s/episodes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-58.75max_-126.58avg_-172.29min__1732547617.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/PC2___-58.75max_-126.58avg_-172.29min__1732547617.model\\assets\n",
      " 74%|#######3  | 221/300 [41:56<16:12, 12.31s/episodes]"
     ]
    }
   ],
   "source": [
    "my_game = AbstractGame(agent)\n",
    "\n",
    "ep_rewards = []\n",
    "ep_goals = []\n",
    "ep_steps = []\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    goals_scored = 0 \n",
    "    steps = 0\n",
    "    step_when_last_goal_scored = 0\n",
    "    previous_goal_distance = 1\n",
    "    # get initial state\n",
    "    my_game.reset()\n",
    "    current_state = my_game.get_current_state()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    while not my_game.done:\n",
    "        episode_step += 1\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            action_enum = agent.get_action(current_state)\n",
    "        else:\n",
    "            action_enum = game_state.GameActions(\n",
    "                np.random.randint(0, NUMBER_OF_ACTIONS)\n",
    "            )\n",
    " \n",
    "        # Do action and get what is the current_observation\n",
    "        my_game.update_frame(action_enum)\n",
    "        \n",
    "        new_state = my_game.get_current_state()        \n",
    "        player_info, goal_info, enemy_info = agent.get_features(new_state)\n",
    "                \n",
    "        is_goal_scored = new_state.current_observation.value == 1\n",
    "        if is_goal_scored:\n",
    "            goals_scored += 1\n",
    "            step_when_last_goal_scored = episode_step \n",
    "            goal_scored_reward = 200 \n",
    "        else:\n",
    "            goal_scored_reward = 0\n",
    "        enemy_attacked_penalty = -100 if new_state.current_observation.value == -1 else 0\n",
    "        goal_distance_penalty = -goal_info[4] * 2 if -goal_info[4] < previous_goal_distance else -2\n",
    "        previous_goal_distance = -goal_info[4]\n",
    "        time_penalty = -0.01  \n",
    "\n",
    "        reward = goal_scored_reward + goal_distance_penalty + time_penalty + enemy_attacked_penalty\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        # update replay memory and train model(s)\n",
    "        player_info, goal_info, enemy_info = agent.get_features(current_state)\n",
    "        current_state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "\n",
    "        player_info, goal_info, enemy_info = agent.get_features(new_state)\n",
    "        new_state_tensor = tf.convert_to_tensor([float(f) for feature_list in [player_info, goal_info, *enemy_info] for f in feature_list])\n",
    "\n",
    "        agent.update_replay_memory((current_state_tensor, action_enum.value, reward, new_state_tensor, my_game.done))\n",
    "        agent.train(my_game.done)\n",
    "        \n",
    "        \n",
    "        current_state = new_state\n",
    "        my_game.__innerState = current_state\n",
    "        if episode_step - step_when_last_goal_scored > 250 and goals_scored == 0:\n",
    "            episode_reward -= 100 # for tensorboard logging, so that there are no falsely high rewards with no goals scored\n",
    "            break # no progress, and hasn't scored a goal in a while\n",
    "        # my_game.draw_frame()\n",
    "    \n",
    "        \n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    ep_goals.append(goals_scored)\n",
    "    ep_steps.append(episode_step)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        average_goals = sum(ep_goals[-AGGREGATE_STATS_EVERY:])/len(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        min_goals = min(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        max_goals = max(ep_goals[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        average_steps = sum(ep_steps[-AGGREGATE_STATS_EVERY:])/len(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        min_steps = min(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        max_steps = max(ep_steps[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        agent.tensorboard.update_stats(\n",
    "            epsilon=epsilon, reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward,\n",
    "            goals_avg=average_goals, goals_min=min_goals, goals_max=max_goals,\n",
    "            steps_avg=average_steps, steps_min=min_steps, steps_max=max_steps\n",
    "        )\n",
    "\n",
    "        if max_reward >= MIN_REWARD: # save model when it's good enough\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "\n",
    "    # decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "        \n",
    "# pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
